{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import normalize\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import requests\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from enum import StrEnum\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_START = \"learning_started\"\n",
    "PROCESSING_FINISH = \"processing_finished\"\n",
    "LEARNING_SUCCESS = \"learning_success\"\n",
    "LEARNING_FAIL = \"learning_failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogStructs:\n",
    "    @staticmethod\n",
    "    def get_time():\n",
    "        return str(datetime.now(timezone.utc))\n",
    "\n",
    "    @staticmethod\n",
    "    def learning_start(line: int, direction: int) -> dict:\n",
    "        return {\n",
    "            \"line\": line,\n",
    "            \"direction\": direction,\n",
    "            \"type\": LEARNING_START,\n",
    "            \"log\": \"START\",\n",
    "            \"timestamp\": LogStructs.get_time(),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def processing_finish(\n",
    "        line: int, direction: int, records_count: int, time_taken: int\n",
    "    ) -> dict:\n",
    "        return {\n",
    "            \"line\": line,\n",
    "            \"direction\": direction,\n",
    "            \"type\": PROCESSING_FINISH,\n",
    "            \"log\": {\"records_count\": records_count, \"time\": time_taken},\n",
    "            \"timestamp\": LogStructs.get_time(),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def learning_success(\n",
    "        line: int, direction: int, parameters_count: int, loss: float, time_taken: int\n",
    "    ) -> dict:\n",
    "        return {\n",
    "            \"line\": line,\n",
    "            \"direction\": direction,\n",
    "            \"type\": LEARNING_SUCCESS,\n",
    "            \"log\": {\n",
    "                \"parameters_count\": parameters_count,\n",
    "                \"loss\": loss,\n",
    "                \"time\": time_taken,\n",
    "            },\n",
    "            \"timestamp\": LogStructs.get_time(),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def learning_fail(line: int, direction: int, exception: str) -> dict:\n",
    "        return {\n",
    "            \"line\": line,\n",
    "            \"direction\": direction,\n",
    "            \"type\": LEARNING_FAIL,\n",
    "            \"log\": exception,\n",
    "            \"timestamp\": LogStructs.get_time(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%config SqlMagic.style = '_DEPRECATED_DEFAULT'\n",
    "%config SqlMagic.autopandas = True\n",
    "%sql postgresql+psycopg://admin:admin@localhost:5432/buses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_NAME = int(os.getenv(\"line_name\"))\n",
    "DIRECTION = int(os.getenv(\"direction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = json.dumps(LogStructs.learning_start(LINE_NAME, DIRECTION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql UPDATE public.ai_tasks SET status = 'STARTED' WHERE line_name = :LINE_NAME AND direction = :DIRECTION\n",
    "%sql INSERT INTO public.ai_logs (line, direction, log_type, log) VALUES (:LINE_NAME, :DIRECTION, :LEARNING_START, :log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data_from_db(line_name: int, optional_direction: str | int):\n",
    "    if isinstance(optional_direction, str): \n",
    "        res = %sql SELECT id FROM public.directions WHERE value = :optional_direction\n",
    "        direction_id = res[0][0]\n",
    "    else:\n",
    "        direction_id = optional_direction\n",
    "    records = %sql SELECT course_loid, day_course_loid, longitude, latitude, angle, reached_meters, order_in_course, last_ping_date FROM \\\n",
    "                public.positions WHERE optional_direction = :direction_id AND line_name = :line_name\n",
    "    course_loids = %sql SELECT DISTINCT(course_loid) FROM public.positions WHERE optional_direction = :direction_id \\\n",
    "                AND line_name = :line_name\n",
    "    return (records, course_loids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_points():\n",
    "    stops = requests.get(\"https://przystanki.bialystok.pl/portal/getStops.json\").json()\n",
    "    parsed = {\n",
    "        point[\"symbol\"]: {\n",
    "            \"latitude\": point[\"latitude\"],\n",
    "            \"longitude\": point[\"longitude\"],\n",
    "        }\n",
    "        for point in stops[\"stopPoints\"]\n",
    "    }\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_courses(course_loids: list[int]):\n",
    "    courses = {}\n",
    "    for loid in course_loids:\n",
    "        course = requests.get(\n",
    "            f\"https://przystanki.bialystok.pl/portal/getRealCourse.json?courseId={loid}\"\n",
    "        ).json()\n",
    "        courses[loid] = {\n",
    "            point[\"orderInCourse\"]: {\n",
    "                \"scheduled_departure\": point[\"scheduledDepartureSec\"],\n",
    "                \"symbol\": point[\"stopPointSymbol\"],\n",
    "            }\n",
    "            for point in course[\"realCourse\"][\"stoppings\"]\n",
    "        }\n",
    "    return courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_objects(data: DataFrame, course_loids: DataFrame) -> torch.Tensor:\n",
    "    parsed_x = []\n",
    "    parsed_x_extended = []\n",
    "    parsed_y = []\n",
    "    courses = {}\n",
    "    counter = 0\n",
    "    stop_points = get_stop_points()\n",
    "    loids = [row.course_loid for _, row in course_loids.iterrows()]\n",
    "    courses = get_real_courses(loids)\n",
    "    for _, row in data.iterrows():\n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "            print(f\"Parsed {counter}\")\n",
    "        predicted_stop_point = (\n",
    "            (row.order_in_course + 1)\n",
    "            if data[\n",
    "                (\n",
    "                    (data[\"day_course_loid\"] == row.day_course_loid)\n",
    "                    & (data[\"order_in_course\"] == (row.order_in_course + 1))\n",
    "                )\n",
    "            ]\n",
    "            .any()\n",
    "            .any()\n",
    "            else row.order_in_course\n",
    "        )\n",
    "        dt = datetime.fromtimestamp(row.last_ping_date / 1000)\n",
    "        time = dt.hour * 3600 + dt.minute * 60 + dt.second\n",
    "        arrived_dt = datetime.fromtimestamp(\n",
    "            data[\n",
    "                (\n",
    "                    (data[\"day_course_loid\"] == row.day_course_loid)\n",
    "                    & (data[\"order_in_course\"] == predicted_stop_point)\n",
    "                )\n",
    "            ]\n",
    "            .nsmallest(1, \"last_ping_date\")\n",
    "            .last_ping_date.item()\n",
    "            / 1000\n",
    "        )\n",
    "        arrived_time = (\n",
    "            arrived_dt.hour * 3600 + arrived_dt.minute * 60 + arrived_dt.second\n",
    "        )\n",
    "        stop_point_data = courses[row.course_loid][predicted_stop_point]\n",
    "        parsed_x.append(\n",
    "            [\n",
    "                row.longitude,\n",
    "                row.latitude,\n",
    "                float(row.angle),\n",
    "                float(row.reached_meters),\n",
    "                float(row.order_in_course),\n",
    "                time,\n",
    "                stop_point_data[\"scheduled_departure\"],\n",
    "                predicted_stop_point,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        parsed_x_extended.append(\n",
    "            [\n",
    "                row.longitude,\n",
    "                row.latitude,\n",
    "                float(row.angle),\n",
    "                float(row.reached_meters),\n",
    "                float(row.order_in_course),\n",
    "                time,\n",
    "                stop_point_data[\"scheduled_departure\"],\n",
    "                predicted_stop_point,\n",
    "                stop_points[stop_point_data[\"symbol\"]][\"latitude\"],\n",
    "                stop_points[stop_point_data[\"symbol\"]][\"longitude\"],\n",
    "            ]\n",
    "        )\n",
    "        parsed_y.append([arrived_time - time])\n",
    "\n",
    "    parsed_x = normalize(torch.tensor(parsed_x, dtype=torch.float32, device=\"cuda\"))\n",
    "    parsed_x_extended = normalize(\n",
    "        torch.tensor(parsed_x_extended, dtype=torch.float32, device=\"cuda\")\n",
    "    )\n",
    "    parsed_y = torch.tensor(parsed_y, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    return zip(parsed_x, parsed_y), zip(parsed_x_extended, parsed_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, num_epochs=100, batch_size=16, learning_rate=0.0011):\n",
    "\n",
    "    learning_start = time.monotonic()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.L1Loss()  # Zmień na MSELoss jeśli Y jest ciągłe\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Przygotowanie do przechowywania historii strat\n",
    "    history = {\"epoch_loss\": []}\n",
    "    min_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward i backward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Sumowanie strat na potrzeby obliczenia średniej\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Oblicz średnią stratę na epokę i dodaj do historii\n",
    "        epoch_loss /= len(dataloader)\n",
    "        history[\"epoch_loss\"].append(epoch_loss)\n",
    "\n",
    "        # Aktualizacja minimalnej straty\n",
    "        if epoch_loss < min_loss:\n",
    "            min_loss = epoch_loss\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    log = json.dumps(LogStructs.learning_success(LINE_NAME, DIRECTION, len(dataset[0][0]), epoch_loss, int(time.monotonic() - learning_start)))\n",
    "    %sql INSERT INTO public.ai_logs (line, direction, log_type, log) VALUES (:LINE_NAME, :DIRECTION, :LEARNING_SUCCESS, :log)\n",
    "\n",
    "    output_dir = f\"{LINE_NAME}_{DIRECTION}/{epoch_loss}_{datetime.now(timezone.utc)}\"\n",
    "    os.makedirs(os.path.dirname(f\"{output_dir}/training_history.json\"), exist_ok=True)\n",
    "    torch.save(model, f\"{output_dir}/model.pt\")\n",
    "    # Zapis historii strat do pliku JSON\n",
    "    with open(f\"{output_dir}/training_history.json\", \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "\n",
    "    data = []\n",
    "    for inputs, targets in dataset:\n",
    "        row = {\"inputs\": inputs.tolist(), \"targets\": targets.tolist()}\n",
    "        data.append(row)\n",
    "    dataset_df = pd.DataFrame(data)\n",
    "    dataset_df.to_csv(f\"{output_dir}/dataset.csv\", index=False)\n",
    "\n",
    "    # Tworzenie wykresu strat i zapis do pliku\n",
    "    plt.plot(range(1, num_epochs + 1), history[\"epoch_loss\"], label=\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{output_dir}/loss_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Generowanie raportu\n",
    "    with open(f\"{output_dir}/training_report.txt\", \"w\") as f:\n",
    "        f.write(\"Model Training Report\\n\")\n",
    "        f.write(\"=====================\\n\")\n",
    "        f.write(f\"Model: {model}\\n\\n\")\n",
    "        f.write(f\"Number of epochs: {num_epochs}\\n\")\n",
    "        f.write(f\"Batch size: {batch_size}\\n\")\n",
    "        f.write(f\"Learning rate: {learning_rate}\\n\")\n",
    "        f.write(f\"Final Loss: {epoch_loss:.4f}\\n\")\n",
    "        f.write(f\"Minimum Loss: {min_loss:.4f}\\n\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataset, batch_size=16):\n",
    "    dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    criterion = nn.L1Loss()  # or nn.MSELoss() depending on your use case\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Calculate loss\n",
    "            test_loss += loss.item() * inputs.size(\n",
    "                0\n",
    "            )  # Accumulate loss (weighted by batch size)\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_dataset)\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_script():\n",
    "    try:\n",
    "        data, loids = get_model_data_from_db(LINE_NAME, DIRECTION)\n",
    "\n",
    "        processing_start = time.monotonic()\n",
    "        prepared, prepared_extended = prepare_objects(data, loids)\n",
    "\n",
    "        data_zipped = list(prepared)\n",
    "        data_extended_zipped = list(prepared_extended)\n",
    "\n",
    "        l = len(data_zipped)\n",
    "        training_data, test_data = data_zipped[:math.floor(0.8*l)], data_zipped[math.floor(0.8*l):]\n",
    "        etraining_data, etest_data = data_extended_zipped[:math.floor(0.8*l)], data_extended_zipped[math.floor(0.8*l):]\n",
    "\n",
    "        log = json.dumps(LogStructs.processing_finish(LINE_NAME, DIRECTION, l, int(time.monotonic() - processing_start)))\n",
    "        %sql INSERT INTO public.ai_logs (line, direction, log_type, log) VALUES (:LINE_NAME, :DIRECTION, :PROCESSING_FINISH, :log)\n",
    "\n",
    "        net = nn.Sequential(\n",
    "            nn.Linear(8, 48),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(48,32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        ).to(device)\n",
    "        net_extraneous = nn.Sequential(\n",
    "            nn.Linear(10, 56),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(56,48),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(48, 32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        ).to(device)\n",
    "\n",
    "        trained = train(net, training_data)\n",
    "        trained_extended = train(net_extraneous, etraining_data)\n",
    "    except Exception as e:\n",
    "        log = LogStructs.learning_fail(LINE_NAME, DIRECTION, str(e))\n",
    "        %sql INSERT INTO public.ai_logs (line, direction, log_type, log) VALUES (:LINE_NAME, :DIRECTION, :LEARNING_FAIL, :log)\n",
    "\n",
    "    %sql UPDATE public.ai_tasks SET status = 'SUCCESS' WHERE line_name = :LINE_NAME AND direction = :DIRECTION;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql UPDATE public.ai_tasks SET status = 'SUCCESS' WHERE line_name = :LINE_NAME AND direction = :DIRECTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
